{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data = pd.read_excel('data.xlsx', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "checking the Project_data columns and number of missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns which do not have influence on the type of risks are dropped. This includes the ID link, the phase of the execution (because we are only predicting at the start anyway), and the names of the people on the project. The country is also dropped since the focus is on Dutch projects.\n",
    "\n",
    "The project number and name are not dropped because this will make it easier to identify the past project that is most similar to a new project. However, these two columns will not be used as features for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data = Project_data.drop(columns=['SharePoint', 'Fase', 'Land', 'Directeur projecten', 'Bedrijfsleider', 'Project Manager', 'Tender Manager'])\n",
    "Project_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project_data.iloc[:,2:].to_csv('Info sheets\\\\Project_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data['Locatie'] = Project_data['Locatie'].astype('category')\n",
    "Project_data['Businessline'] = Project_data['Businessline'].astype('category')\n",
    "Project_data['Opdrachtgever'] = Project_data['Opdrachtgever'].astype('category')\n",
    "Project_data['Type Opdrachtgever'] = Project_data['Type Opdrachtgever'].astype('category')\n",
    "Project_data['EMVI'] = Project_data['EMVI'].astype('category')\n",
    "Project_data['Contracttype'] = Project_data['Contracttype'].astype('category')\n",
    "Project_data['BKN waarde'] = Project_data['BKN waarde'].astype('category')\n",
    "\n",
    "\n",
    "Project_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw only 'prekwalificatie uitslag' has NA's. This could be because not every project has a pre-qualification process before the tender. Since the type of risks are not really dependent on when this qualification takes place but might be influenced by if there is a qualification process or not, this variable will be converted to a dummy variable. When it is equal to 1 it did occur, when equal to 0 it did not occur.\n",
    "\n",
    "The date of submitting the tender should not have much of an affect on the type of risks in a project so this will be removed too, and the same goes for 'Voormenen tot gunning'.\n",
    "\n",
    "The start and end date could have an effect as certain seasons may deal with different risks. Another thing that could have an effect is the total time for realisation, so an extra column will be created to determine the planned project time (the differnece between start and end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dummy variable\n",
    "Project_data['prekwalificatie'] = np.where(Project_data['Prekwalificatie uitslag'].isna(), 0, 1)\n",
    "\n",
    "#turning it into a category\n",
    "Project_data['prekwalificatie'] = Project_data['prekwalificatie'].astype('category')\n",
    "\n",
    "#dropping columns\n",
    "Project_data = Project_data.drop(columns=['Indienen tender', 'Voornemen tot gunning', 'Prekwalificatie uitslag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new variable project duration\n",
    "\n",
    "Project_data['Project duration'] = (Project_data['Eind realisatie'] - Project_data['Start realisatie']).dt.days\n",
    "\n",
    "print(Project_data['Project duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be seen that for one it is negative. Upon closer inspection, it seems like this is because the year has been wrongly interpreted by excel as 1930 instead of 2030. Lets change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Project_data['Project duration'])):\n",
    "    if(Project_data['Project duration'][i] < 0):\n",
    "        Project_data['Eind realisatie'][i] = Project_data['Eind realisatie'][i] + pd.DateOffset(years=100)\n",
    "\n",
    "Project_data['Project duration'] = (Project_data['Eind realisatie'] - Project_data['Start realisatie']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split up the datetimes to month and year so they are easier to compare. Days should not make that much of a difference as too specific for year long projects. The information is also not lost since it is still included in the 'Project Duration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data['Start Year'] = Project_data['Start realisatie'].dt.year\n",
    "Project_data['Start Month'] = Project_data['Start realisatie'].dt.month\n",
    "\n",
    "Project_data['End Year'] = Project_data['Eind realisatie'].dt.year\n",
    "Project_data['End Month'] = Project_data['Eind realisatie'].dt.month\n",
    "\n",
    "Project_data = Project_data.drop(columns=['Start realisatie', 'Eind realisatie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data['Start Year'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data.iloc[4,4] = 'Rijkswaterstaat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimpy import skim\n",
    "\n",
    "skim(Project_data.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data['HLC_std'].mean()\n",
    "Project_data['HLC_std'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot for Project Duration\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(Project_data['Project duration'], bins=20, edgecolor='black')\n",
    "plt.xlabel('Project duration', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=17)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Plot for Area\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(Project_data['Area'], bins=20, edgecolor='black')\n",
    "plt.xlabel('Area', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Adding (a) and (b) titles below the graphs\n",
    "plt.figtext(0.25, 0.01, '(a) Histogram of Project duration', ha='center', fontsize=20)\n",
    "plt.figtext(0.75, 0.01, '(b) Histogram of Area', ha='center', fontsize=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot for Start Month\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(Project_data['Start Month'], bins=10, edgecolor='black')\n",
    "plt.xlabel('Start Month', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=17)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Plot for End Month\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(Project_data['End Month'], bins=10, edgecolor='black')\n",
    "plt.xlabel('End Month', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Adding (a) and (b) titles below the graphs\n",
    "plt.figtext(0.25, 0.01, '(a) Histogram of Start Month', ha='center', fontsize=20)\n",
    "plt.figtext(0.75, 0.01, '(b) Histogram of End Month', ha='center', fontsize=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot for HLC_mean\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(Project_data['HLC_mean'], bins=10, edgecolor='black')\n",
    "plt.xlabel('HLC_mean', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=17)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Plot for HLC_std\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(Project_data['HLC_std'], bins=10, edgecolor='black')\n",
    "plt.xlabel('HLC_std', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylabel('')  # Remove y-axis label\n",
    "\n",
    "# Adding (a) and (b) titles below the graphs\n",
    "plt.figtext(0.25, 0.01, '(a) Histogram of HLC_mean', ha='center', fontsize=20)\n",
    "plt.figtext(0.75, 0.01, '(b) Histogram of HLC_std', ha='center', fontsize=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are more projects with prekwalificatie than without. Project duration has a high standard deviation due to a big right tail and the average is around 1500 days. The start year and month have less deviation than the end year and month, which makes somewhat sense as there is more control oer when to start than when to end.\n",
    "\n",
    "The location for these 25 projects are all unique so this will not help with the modelling. The other categorical variables do have at least one category which occurs more than one since the unique observations are less than the total number of observations. This is what was expected/wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of bag method for splitting!!!\n",
    "training on all and testing on 1 for every project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I use the original probability instead of classification and make it a regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering_data = pd.read_excel('clustering_data.xlsx', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering_data['Project'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "# Function to find closest match\n",
    "def find_closest_match(abbrev_name, full_names):\n",
    "    matches = process.extractOne(abbrev_name, full_names)\n",
    "    return matches[0]\n",
    "\n",
    "# Find closest full names for each abbreviated name\n",
    "closest_matches = [find_closest_match(name, Project_data['Projectnaam']) for name in Clustering_data['Project'].unique()]\n",
    "\n",
    "print(closest_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the project in clustering data with the closest match\n",
    "Clustering_data['Project'] = Clustering_data['Project'].replace(Clustering_data['Project'].unique(), closest_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using one hot encoding for Cluster_Labels \n",
    "Cluster_labels = pd.get_dummies(Clustering_data['Cluster_Labels'], prefix='Cluster')\n",
    "Label_data = pd.concat([Clustering_data['Project'], Cluster_labels], axis=1)\n",
    "\n",
    "concat_labels = Label_data.groupby('Project').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with the project data on Projectnaam where concat_labels is index \n",
    "Project_labels = Project_data.set_index('Projectnaam')\n",
    "Project_labels = Project_labels.join(concat_labels, how='inner')\n",
    "\n",
    "Project_labels = Project_labels.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "#print(\"imbalanced-learn version:\", imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support, make_scorer, fbeta_score, precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the target variables to a list\n",
    "target_variables = Project_labels.loc[:, 'Cluster_Financieel_0':].values.tolist()\n",
    "\n",
    "#convert to matrix\n",
    "y = np.array(target_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the target variable\n",
    "X = Project_labels.loc[:, 'Locatie':'End Month']\n",
    "\n",
    "numerical_features = X.select_dtypes(include=['int64', 'Int32', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn all categorical variables into dummy variables\n",
    "X_dummies = pd.get_dummies(X[categorical_features])\n",
    "\n",
    "#remove the unecessary dummies for EMVI and prekwalificatie\n",
    "X_dummies = X_dummies.drop(columns=['EMVI_Nee', 'prekwalificatie_0'])\n",
    "\n",
    "#concatenate the numerical and dummy variables\n",
    "X_transformed = pd.concat([X[numerical_features], X_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need ot get the groups of one hot encoded variables\n",
    "from collections import defaultdict\n",
    "\n",
    "def identify_one_hot_encoded_groups(X_transformed):\n",
    "    \"\"\"\n",
    "    Identify groups of one-hot encoded categorical features in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_transformed: Features data (pandas DataFrame)\n",
    "    \n",
    "    Returns:\n",
    "    - List of lists, where each sublist contains column names of one-hot encoded features belonging to the same categorical variable\n",
    "    \"\"\"\n",
    "    one_hot_groups = defaultdict(list)\n",
    "    \n",
    "    for col in X_transformed.columns:\n",
    "        if '_' in col:\n",
    "            prefix = col.rsplit('_', 1)[0]\n",
    "            one_hot_groups[prefix].append(col)\n",
    "    \n",
    "    return list(one_hot_groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_groups = identify_one_hot_encoded_groups(X_transformed)\n",
    "\n",
    "col_to_index = {col: idx for idx, col in enumerate(X_transformed.columns)} \n",
    "one_hot_group_indices = [[col_to_index[col] for col in group] for group in one_hot_groups] if one_hot_groups else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WATCH OUT HOW TO SPLIT\n",
    "#create test data from index 0, 14, 17, 19\n",
    "X_test_unchanged = X_transformed.iloc[[0,14,17,19],:]\n",
    "X_train_unchanged = X_transformed.drop([0,14,17,19])\n",
    "\n",
    "# Now scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaler = scaler.fit(X_train_unchanged[numerical_features])\n",
    "\n",
    "X_test_scaled = X_train_scaler.transform(X_test_unchanged[numerical_features])\n",
    "X_test_numeric = pd.DataFrame(X_test_scaled, columns=numerical_features, index=X_test_unchanged.index)\n",
    "X_test = pd.concat([X_test_numeric, X_dummies.iloc[[0,14,17,19],:]], axis=1)\n",
    "\n",
    "X_train_scaled = X_train_scaler.transform(X_train_unchanged[numerical_features])\n",
    "X_train_numeric = pd.DataFrame(X_train_scaled, columns=numerical_features, index=X_train_unchanged.index)\n",
    "X_train = pd.concat([X_train_numeric, X_dummies.drop([0,14,17,19])], axis=1)\n",
    "\n",
    "y_test = y[[0,14,17,19],:]\n",
    "y_train = np.delete(y, [0,14,17,19], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y_train[:, 45] == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def MLSMOTE(X, y, k=5, one_hot_group_indices=None):\n",
    "    \"\"\"\n",
    "    Perform MLSMOTE (Multilabel Synthetic Minority Over-sampling Technique)\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Features data (numpy array)\n",
    "    - y: Multi-label targets (numpy array)\n",
    "    - k: Number of nearest neighbors to use for synthetic instance generation\n",
    "    - one_hot_group_indices: List of lists containing indices of one-hot encoded feature columns\n",
    "    \n",
    "    Returns:\n",
    "    - X_resampled: Resampled features\n",
    "    - y_resampled: Resampled targets\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "\n",
    "    # Ensure y is a numpy array\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Calculate the initial imbalance ratio for each label\n",
    "    label_counts = np.sum(y, axis=0)\n",
    "    max_count = np.max(label_counts)\n",
    "    \n",
    "    # Nearest Neighbors model\n",
    "    nn = NearestNeighbors(n_neighbors=k+1)  # +1 because the instance itself is included\n",
    "    nn.fit(X)\n",
    "    \n",
    "    synthetic_X = []\n",
    "    synthetic_y = []\n",
    "\n",
    "    # Track the number of synthetic samples generated for each label\n",
    "    synthetic_label_counts = np.copy(label_counts)\n",
    "\n",
    "    for label in range(y.shape[1]):\n",
    "        while (synthetic_label_counts[label] < max_count) and (synthetic_label_counts[label] != 0):\n",
    "            # Get instances with the current label\n",
    "            minority_instances = np.where(y[:, label] == 1)[0]\n",
    "            \n",
    "            # Randomly pick an instance from minority instances\n",
    "            instance = np.random.choice(minority_instances)\n",
    "            \n",
    "            # Find the k-nearest neighbors\n",
    "            neighbors = nn.kneighbors([X[instance]], return_distance=False).flatten()\n",
    "            neighbors = neighbors[neighbors != instance]  # Exclude the instance itself\n",
    "            \n",
    "            # Randomly pick one neighbor\n",
    "            neighbor = np.random.choice(neighbors)\n",
    "            \n",
    "            # Create synthetic instance\n",
    "            synthetic_instance = np.zeros(X.shape[1])\n",
    "            \n",
    "            for group in one_hot_group_indices:\n",
    "                # For one-hot encoded groups, choose one value from the group\n",
    "                chosen_instance = np.random.choice([instance, neighbor])\n",
    "                synthetic_instance[group] = X[chosen_instance, group]\n",
    "            \n",
    "            for i in range(X.shape[1]):\n",
    "                if not any(i in group for group in one_hot_group_indices):\n",
    "                    # For numerical features, interpolate\n",
    "                    diff = X[neighbor, i] - X[instance, i]\n",
    "                    gap = np.random.random()\n",
    "                    synthetic_instance[i] = X[instance, i] + gap * diff\n",
    "            \n",
    "            # Create synthetic labelset\n",
    "            synthetic_labelset = y[instance] | y[neighbor]\n",
    "            \n",
    "\n",
    "            # Append synthetic instance and labelset\n",
    "            synthetic_X.append(synthetic_instance)\n",
    "            synthetic_y.append(synthetic_labelset)\n",
    "            \n",
    "            # Update synthetic label counts\n",
    "            synthetic_label_counts += synthetic_labelset\n",
    "\n",
    "    synthetic_X = np.array(synthetic_X)\n",
    "    synthetic_y = np.array(synthetic_y)\n",
    "    \n",
    "    # Combine original data with synthetic data\n",
    "    X_resampled = np.vstack((X, synthetic_X))\n",
    "    y_resampled = np.vstack((y, synthetic_y))\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply MLSMOTE to your training data\n",
    "X_train_resampled, y_train_resampled = MLSMOTE(X_train, y_train, k=5, one_hot_group_indices=one_hot_group_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retry with max extra label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare to not using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of instances for each label\n",
    "print(\"Number of instances for each label in the resampled training set:\")\n",
    "print(np.sum(y_train_resampled, axis=0))\n",
    "print(np.sum(y_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #bootstrapping to get more data\n",
    "# X_train_resampled, y_train_resampled = resample(X_train, y_train, replace=True, n_samples=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://mermaid.ink/img/pako:eNqdlFtv2jAUx7-K5b4GRG6M5GFSIRSoYEId28OWPbjxSeMRnMh2Vijiu9e5UbohNZofIufk_zu3HPuIo4wC9vGTIHmCNuOQhxwhWTzWhnVCJJg_Q_zA5BbNRFbkjD_V5hD_KrUIUSYgUizjaPlQW241MWdSZYJFJEVrkf3WClQ5CTIpGQipcdTrfUZjrd3c9RbBHVIZ-q51mnoBtIG9ajUTrfnGZZGD-MMkUDRJC6lA6FxaSfAuSXlOLqi-zvTXJeNbdCEpwzWZtXLgtKKqLlxrhdVGWQugrC76o2bUz_n1lix4nIkdKYG2kuklNq1MCw03EXXxlzVoGn2B59bfOY17TVzY_4nTL73W0lkdVQOrIlWsl5JHSNGKRAnjgJZABC__-UoPSvq-T2WHmi6pQwrNtKCYpal_E9uxEw8NqUS2Bf_Gtu1m33tmVCW-le__Iq2GBAtccDuSt224anWExg1ECYzi6BJ600w6aIJGEzk0jpzrmmkHP7MOfub_U-iig-P7Dx1TIhMiBDn4yK4OBkLYwDvQA8WovjyO1TBglcBOHwRfbynERA9TiEN-0lJSqOzrgUfYV6IAAxc5JQoCRvTR2mE_Jqk8W6eU6UNyNkL1uqpvqeqyMnBO-I8sewP1O_aPeI_9Ud8cuKZl2u7AHpqm-cnAB-x7fc-zXc8djryBO_Ac72Tgl8qB2R_Uy7ZHruMMreHpFZHepDo?type=png)](https://mermaid-live-editor.fly.dev/edit#pako:eNqdlFtv2jAUx7-K5b4GRG6M5GFSIRSoYEId28OWPbjxSeMRnMh2Vijiu9e5UbohNZofIufk_zu3HPuIo4wC9vGTIHmCNuOQhxwhWTzWhnVCJJg_Q_zA5BbNRFbkjD_V5hD_KrUIUSYgUizjaPlQW241MWdSZYJFJEVrkf3WClQ5CTIpGQipcdTrfUZjrd3c9RbBHVIZ-q51mnoBtIG9ajUTrfnGZZGD-MMkUDRJC6lA6FxaSfAuSXlOLqi-zvTXJeNbdCEpwzWZtXLgtKKqLlxrhdVGWQugrC76o2bUz_n1lix4nIkdKYG2kuklNq1MCw03EXXxlzVoGn2B59bfOY17TVzY_4nTL73W0lkdVQOrIlWsl5JHSNGKRAnjgJZABC__-UoPSvq-T2WHmi6pQwrNtKCYpal_E9uxEw8NqUS2Bf_Gtu1m33tmVCW-le__Iq2GBAtccDuSt224anWExg1ECYzi6BJ600w6aIJGEzk0jpzrmmkHP7MOfub_U-iig-P7Dx1TIhMiBDn4yK4OBkLYwDvQA8WovjyO1TBglcBOHwRfbynERA9TiEN-0lJSqOzrgUfYV6IAAxc5JQoCRvTR2mE_Jqk8W6eU6UNyNkL1uqpvqeqyMnBO-I8sewP1O_aPeI_9Ud8cuKZl2u7AHpqm-cnAB-x7fc-zXc8djryBO_Ac72Tgl8qB2R_Uy7ZHruMMreHpFZHepDo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_HLC = X_train\n",
    "X_train_noHLC = X_train.drop(columns=['HLC_mean', 'HLC_std'])\n",
    "X_test_HLC = X_test\n",
    "X_test_noHLC = X_test.drop(columns=['HLC_mean', 'HLC_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of labels for each test observation\n",
    "y_test.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "#tuning parameters\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 10, 15, 20],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Define the F2 score as the scoring metric\n",
    "f_scorer = make_scorer(fbeta_score, beta=1, average='macro', zero_division=1)\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use weighted because accounts for label impabalance\n",
    "\n",
    "zero division = 1 because it means its all predicted correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noHLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid_search = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=loo, scoring=f_scorer, n_jobs=-2)\n",
    "knn_grid_search.fit(X_train_noHLC, y_train)\n",
    "print(\"Best parameters:\", knn_grid_search.best_params_)\n",
    "print(\"Best score:\", knn_grid_search.best_score_)\n",
    "\n",
    "knn_best_model_noHLC = knn_grid_search.best_estimator_\n",
    "knn_y_pred_noHLC = knn_best_model_noHLC.predict(X_test_noHLC)\n",
    "knn_metrics_noHLC = precision_recall_fscore_support(y_test, knn_y_pred_noHLC, average='macro', zero_division=1, beta=1)\n",
    "print(\"Recall on test set:\", knn_metrics_noHLC[1]) #0.564\n",
    "print(\"Precision on test set:\", knn_metrics_noHLC[0])#930\n",
    "print(\"F Score on test set:\", knn_metrics_noHLC[2])#0.531"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid_search = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=loo, scoring=f_scorer, n_jobs=-2)\n",
    "knn_grid_search.fit(X_train_HLC, y_train)\n",
    "print(\"Best parameters:\", knn_grid_search.best_params_)\n",
    "print(\"Best score:\", knn_grid_search.best_score_)\n",
    "\n",
    "knn_best_model_HLC = knn_grid_search.best_estimator_\n",
    "knn_y_pred_HLC = knn_best_model_HLC.predict(X_test_HLC)\n",
    "knn_metrics_HLC = precision_recall_fscore_support(y_test, knn_y_pred_HLC, average='macro', zero_division=1, beta=1)\n",
    "print(\"Recall on test set:\", knn_metrics_HLC[1]) #0.565\n",
    "print(\"Precision on test set:\", knn_metrics_HLC[0])#0.930\n",
    "print(\"F Score on test set:\", knn_metrics_HLC[2])#0.531"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero division = 1 makes sense for recall!\n",
    "\n",
    "This can only happen when there are no real positive values and therefore all real ones are predicted correctly (sometimes still means that some are wrongly predicted as positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noHLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting probability of labels\n",
    "knn_proba_pred_noHLC = knn_best_model_noHLC.predict_proba(X_test_noHLC)\n",
    "\n",
    "# Initialize a list to store the top 20 labels for each observation\n",
    "top_20_indices_per_observation_knn_noHLC = []\n",
    "\n",
    "# For each test observation, extract the top 20 labels\n",
    "for i in range(X_test_noHLC.shape[0]):\n",
    "    # Get the predicted probabilities for the i-th observation\n",
    "    proba = np.array([knn_proba_pred_noHLC[j][i, 0] for j in range(len(knn_proba_pred_noHLC))])\n",
    "    \n",
    "    # Get the indices of the top 20 labels sorted by probability\n",
    "    top_20_knn_indices_noHLC = np.argsort(proba)[:20][::-1]\n",
    "    \n",
    "    # Store the top 20 labels\n",
    "    top_20_indices_per_observation_knn_noHLC.append(top_20_knn_indices_noHLC)\n",
    "\n",
    "    top_20_knn_labels_noHLC = [concat_labels.columns[idx] for idx in top_20_knn_indices_noHLC]\n",
    "\n",
    "    print(top_20_knn_labels_noHLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_top_20_knn_noHLC = np.zeros((X_test_noHLC.shape[0], concat_labels.shape[1]), dtype=bool)\n",
    "for i, top_20_knn_indices_noHLC in enumerate(top_20_indices_per_observation_knn_noHLC):\n",
    "    y_pred_top_20_knn_noHLC[i, top_20_knn_indices_noHLC] = True\n",
    "\n",
    "# Calculate precision for the top 20 predicted labels\n",
    "precision = precision_score(y_test, y_pred_top_20_knn_noHLC, average='samples', zero_division=1)\n",
    "print(f\"Precision for top 20 labels: {precision}\") #0.500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting probability of labels\n",
    "knn_proba_pred_HLC = knn_best_model_HLC.predict_proba(X_test_HLC)\n",
    "\n",
    "# Initialize a list to store the top 20 labels for each observation\n",
    "top_20_indices_per_observation_knn_HLC = []\n",
    "\n",
    "# For each test observation, extract the top 20 labels\n",
    "for i in range(X_test_HLC.shape[0]):\n",
    "    # Get the predicted probabilities for the i-th observation\n",
    "    proba = np.array([knn_proba_pred_HLC[j][i, 0] for j in range(len(knn_proba_pred_HLC))])\n",
    "    \n",
    "    # Get the indices of the top 20 labels sorted by probability\n",
    "    top_20_knn_indices_HLC = np.argsort(proba)[:20][::-1]\n",
    "    \n",
    "    # Store the top 20 labels\n",
    "    top_20_indices_per_observation_knn_HLC.append(top_20_knn_indices_HLC)\n",
    "\n",
    "    top_20_knn_labels_HLC = [concat_labels.columns[idx] for idx in top_20_knn_indices_HLC]\n",
    "\n",
    "    print(top_20_knn_labels_HLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_top_20_knn_HLC = np.zeros((X_test_HLC.shape[0], concat_labels.shape[1]), dtype=bool)\n",
    "for i, top_20_knn_indices_HLC in enumerate(top_20_indices_per_observation_knn_HLC):\n",
    "    y_pred_top_20_knn_HLC[i, top_20_knn_indices_HLC] = True\n",
    "\n",
    "# Calculate precision for the top 20 predicted labels\n",
    "precision = precision_score(y_test, y_pred_top_20_knn_HLC, average='samples', zero_division=1)\n",
    "print(f\"Precision for top 20 labels: {precision}\") #0.500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best'],\n",
    "    'max_depth': [None, 3, 4, 5, 7],\n",
    "    'min_samples_split': [2, 4, 5, 6, 7],\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noHLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "dt_grid_search = GridSearchCV(DecisionTreeClassifier(random_state=0), dt_param_grid, cv=loo, scoring=f_scorer, n_jobs=-2)\n",
    "dt_grid_search.fit(X_train_noHLC, y_train)\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(\"Best parameters:\", dt_grid_search.best_params_)\n",
    "print(\"Best score:\", dt_grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "dt_best_model_noHLC = dt_grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels for the test set\n",
    "dt_y_pred_noHLC = dt_best_model_noHLC.predict(X_test_noHLC)\n",
    "\n",
    "# Evaluate the model using precision, recall, and F2 score\n",
    "dt_metrics_noHLC = precision_recall_fscore_support(y_test, dt_y_pred_noHLC, average='samples', zero_division=1, beta=1)\n",
    "print(\"Recall on test set:\", dt_metrics_noHLC[1])  # 0.214\n",
    "print(\"Precision on test set:\", dt_metrics_noHLC[0])  # 0.529\n",
    "print(\"F Score on test set:\", dt_metrics_noHLC[2])  # 0.252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "dt_grid_search = GridSearchCV(DecisionTreeClassifier(random_state=0), dt_param_grid, cv=loo, scoring=f_scorer, n_jobs=-2)\n",
    "dt_grid_search.fit(X_train_HLC, y_train)\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(\"Best parameters:\", dt_grid_search.best_params_)\n",
    "print(\"Best score:\", dt_grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "dt_best_model_HLC = dt_grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels for the test set\n",
    "dt_y_pred_HLC = dt_best_model_HLC.predict(X_test_HLC)\n",
    "\n",
    "# Evaluate the model using precision, recall, and F2 score\n",
    "dt_metrics_HLC = precision_recall_fscore_support(y_test, dt_y_pred_HLC, average='macro', zero_division=1, beta=1)\n",
    "print(\"Recall on test set:\", dt_metrics_HLC[1])  # 0.536\n",
    "print(\"Precision on test set:\", dt_metrics_HLC[0])  # 0.911\n",
    "print(\"F Score on test set:\", dt_metrics_HLC[2])  # 0.507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import html\n",
    "\n",
    "# Escape special characters in feature names and class names\n",
    "escaped_feature_names = [html.escape(name) for name in X_train_HLC.columns]\n",
    "escaped_class_names = [range(y_train.shape[1])]\n",
    "\n",
    "# Custom export_graphviz function to exclude the 'value' parameter\n",
    "def export_graphviz_custom(decision_tree, out_file=None, feature_names=None, class_names=None, filled=True, rounded=True, special_characters=True):\n",
    "    from sklearn.tree import _tree\n",
    "\n",
    "    def node_to_str(tree, node_id):\n",
    "        if not isinstance(tree, _tree.Tree):\n",
    "            raise ValueError('tree must be a _tree.Tree instance')\n",
    "        \n",
    "        if tree.children_left[node_id] == _tree.TREE_LEAF:\n",
    "            return 'Leaf'\n",
    "        else:\n",
    "            feature = feature_names[tree.feature[node_id]]\n",
    "            threshold = tree.threshold[node_id]\n",
    "            return f'{feature} <= {threshold:.2f}'\n",
    "\n",
    "    def recurse(tree, node_id, parent=None):\n",
    "        name = f'node{node_id}'\n",
    "        if tree.children_left[node_id] != _tree.TREE_LEAF:\n",
    "            left_child = tree.children_left[node_id]\n",
    "            right_child = tree.children_right[node_id]\n",
    "            node_str = node_to_str(tree, node_id)\n",
    "            dot.node(name, label=node_str, shape='box', style='filled', fillcolor='lightgrey')\n",
    "            if parent is not None:\n",
    "                dot.edge(parent, name)\n",
    "            recurse(tree, left_child, parent=name)\n",
    "            recurse(tree, right_child, parent=name)\n",
    "        else:\n",
    "            leaf_label = f'Leaf'\n",
    "            dot.node(name, label=leaf_label, shape='box', style='filled', fillcolor='lightblue')\n",
    "            if parent is not None:\n",
    "                dot.edge(parent, name)\n",
    "\n",
    "    dot = graphviz.Digraph()\n",
    "    recurse(decision_tree.tree_, 0)\n",
    "    dot.render(out_file, format='png')\n",
    "    return dot\n",
    "\n",
    "# Visualize the decision tree using the custom export_graphviz function\n",
    "dot = export_graphviz_custom(dt_best_model_HLC, \n",
    "                             out_file=\"decision_tree\", \n",
    "                             feature_names=escaped_feature_names, \n",
    "                             class_names=escaped_class_names)\n",
    "\n",
    "# View the generated image\n",
    "from IPython.display import Image\n",
    "Image(filename='decision_tree.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of risk labels per project\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noHLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting probability of labels\n",
    "dt_proba_pred_noHLC = dt_best_model_noHLC.predict_proba(X_test_noHLC)\n",
    "\n",
    "# Initialize a list to store the top 20 labels for each observation\n",
    "top_20_indices_per_observation_dt_noHLC = []\n",
    "\n",
    "# For each test observation, extract the top 20 labels\n",
    "for i in range(X_test_noHLC.shape[0]):\n",
    "    # Get the predicted probabilities for the i-th observation\n",
    "    proba = np.array([dt_proba_pred_noHLC[j][i, 0] for j in range(len(dt_proba_pred_noHLC))])\n",
    "    \n",
    "    # Get the indices of the top 20 labels sorted by probability\n",
    "    top_20_dt_indices_noHLC = np.argsort(proba)[:20][::-1]\n",
    "    \n",
    "    # Store the top 20 labels\n",
    "    top_20_indices_per_observation_dt_noHLC.append(top_20_dt_indices_noHLC)\n",
    "\n",
    "    top_20_dt_labels_noHLC = [concat_labels.columns[idx] for idx in top_20_dt_indices_noHLC]\n",
    "\n",
    "    print(top_20_dt_labels_noHLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_top_20_dt_noHLC = np.zeros((X_test_noHLC.shape[0], concat_labels.shape[1]), dtype=bool)\n",
    "for i, top_20_dt_indices_noHLC in enumerate(top_20_indices_per_observation_dt_noHLC):\n",
    "    y_pred_top_20_dt_noHLC[i, top_20_dt_indices_noHLC] = True\n",
    "\n",
    "# Calculate precision for the top 20 predicted labels\n",
    "precision = precision_score(y_test, y_pred_top_20_dt_noHLC, average='samples', zero_division=1)\n",
    "print(f\"Precision for top 20 labels: {precision}\") #0.525"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting probability of labels\n",
    "dt_proba_pred_HLC = dt_best_model_HLC.predict_proba(X_test_HLC)\n",
    "\n",
    "# Initialize a list to store the top 20 labels for each observation\n",
    "top_20_indices_per_observation_dt_HLC = []\n",
    "\n",
    "# For each test observation, extract the top 20 labels\n",
    "for i in range(X_test_HLC.shape[0]):\n",
    "    # Get the predicted probabilities for the i-th observation\n",
    "    proba = np.array([dt_proba_pred_HLC[j][i, 0] for j in range(len(dt_proba_pred_HLC))])\n",
    "    \n",
    "    # Get the indices of the top 20 labels sorted by probability\n",
    "    top_20_dt_indices_HLC = np.argsort(proba)[:20][::-1]\n",
    "    \n",
    "    # Store the top 20 labels\n",
    "    top_20_indices_per_observation_dt_HLC.append(top_20_dt_indices_HLC)\n",
    "\n",
    "    top_20_labels_HLC = [concat_labels.columns[idx] for idx in top_20_dt_indices_HLC]\n",
    "\n",
    "    print(top_20_labels_HLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_top_20_dt_HLC = np.zeros((X_test_HLC.shape[0], concat_labels.shape[1]), dtype=bool)\n",
    "for i, top_20_dt_indices_HLC in enumerate(top_20_indices_per_observation_dt_HLC):\n",
    "    y_pred_top_20_dt_HLC[i, top_20_dt_indices_HLC] = True\n",
    "\n",
    "# Calculate precision for the top 20 predicted labels\n",
    "precision = precision_score(y_test, y_pred_top_20_dt_HLC, average='samples', zero_division=1)\n",
    "print(f\"Precision for top 20 labels: {precision}\") #0.525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision score for top 20 labels for each of the test cases\n",
    "for i in range(len(y_pred_top_20_dt_HLC)):\n",
    "    print(precision_score(y_test[i,:], y_pred_top_20_dt_HLC[i,:]), np.sum(y_test, axis=1)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_pred_top_20_dt_noHLC, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which variables are the most important\n",
    "# Get feature importances\n",
    "dt_feature_importances = dt_best_model_HLC.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importances\n",
    "dt_feature_importances_df = pd.DataFrame({'Feature': X_train_HLC.columns, 'Importance': dt_feature_importances})\n",
    "dt_feature_importances_df = dt_feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the top 10 most important features\n",
    "print(\"Top 10 most important features:\")\n",
    "print(dt_feature_importances_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT with MLSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "dt_grid_search = GridSearchCV(DecisionTreeClassifier(random_state=0), dt_param_grid, cv=loo, scoring=f_scorer, n_jobs=-2)\n",
    "dt_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(\"Best parameters:\", dt_grid_search.best_params_)\n",
    "print(\"Best score:\", dt_grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "dt_best_model_MLSMOTE = dt_grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels for the test set\n",
    "dt_y_pred_MLSMOTE = dt_best_model_HLC.predict(X_test)\n",
    "\n",
    "# Evaluate the model using precision, recall, and F2 score\n",
    "dt_metrics_MLSMOTE = precision_recall_fscore_support(y_test, dt_y_pred_MLSMOTE, average='macro', zero_division=1, beta=1)\n",
    "print(\"Recall on test set:\", dt_metrics_MLSMOTE[1])  # 0.536\n",
    "print(\"Precision on test set:\", dt_metrics_MLSMOTE[0])  # 0.911\n",
    "print(\"F2 Score on test set:\", dt_metrics_MLSMOTE[2])  # 0.507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting probability of labels\n",
    "dt_proba_pred_MLSMOTE = dt_best_model_MLSMOTE.predict_proba(X_test_HLC)\n",
    "\n",
    "# Initialize a list to store the top 20 labels for each observation\n",
    "top_20_indices_per_observation_dt_MLSMOTE = []\n",
    "\n",
    "# For each test observation, extract the top 20 labels\n",
    "for i in range(X_test.shape[0]):\n",
    "    # Get the predicted probabilities for the i-th observation\n",
    "    proba = np.array([dt_proba_pred_MLSMOTE[j][i, 0] for j in range(len(dt_proba_pred_MLSMOTE))])\n",
    "    \n",
    "    # Get the indices of the top 20 labels sorted by probability\n",
    "    top_20_dt_indices_MLSMOTE = np.argsort(proba)[:20][::-1]\n",
    "    \n",
    "    # Store the top 20 labels\n",
    "    top_20_indices_per_observation_dt_MLSMOTE.append(top_20_dt_indices_HLC)\n",
    "\n",
    "    top_20_dt_labels_MLSMOTE = [concat_labels.columns[idx] for idx in top_20_dt_indices_HLC]\n",
    "\n",
    "    print(top_20_dt_labels_MLSMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_top_20_dt_MLSMOTE = np.zeros((X_test.shape[0], concat_labels.shape[1]), dtype=bool)\n",
    "for i, top_20_dt_indices_MLSMOTE in enumerate(top_20_indices_per_observation_dt_MLSMOTE):\n",
    "    y_pred_top_20_dt_MLSMOTE[i, top_20_dt_indices_MLSMOTE] = True\n",
    "\n",
    "# Calculate precision for the top 20 predicted labels\n",
    "precision = precision_score(y_test, y_pred_top_20_dt_MLSMOTE, average='samples', zero_division=1)\n",
    "print(f\"Precision for top 20 labels: {precision}\") # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [None, 3, 10, 15],\n",
    "    'min_samples_split': [2, 3, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2]    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noHLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=0), rf_param_grid, cv=loo, scoring=f_scorer, n_jobs=-2)\n",
    "rf_grid_search.fit(X_train_noHLC, y_train)\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(\"Best parameters:\", rf_grid_search.best_params_)\n",
    "print(\"Best score:\", rf_grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "rf_best_model_noHLC = rf_grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels for the test set\n",
    "rf_y_pred_noHLC = rf_best_model_noHLC.predict(X_test_noHLC)\n",
    "\n",
    "# Evaluate the model using precision, recall, and F2 score\n",
    "rf_metrics_noHLC = precision_recall_fscore_support(y_test, rf_y_pred_noHLC, average='macro', zero_division=1, beta=1)\n",
    "print(\"Recall on test set:\", rf_metrics_noHLC[1])  # 0.550\n",
    "print(\"Precision on test set:\", rf_metrics_noHLC[0])  # 0.952\n",
    "print(\"F2 Score on test set:\", rf_metrics_noHLC[2])  # 0.516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting probability of labels\n",
    "rf_proba_pred_noHLC = rf_best_model_noHLC.predict_proba(X_test_noHLC)\n",
    "\n",
    "# Initialize a list to store the top 20 labels for each observation\n",
    "top_20_indices_per_observation_rf_noHLC = []\n",
    "\n",
    "# For each test observation, extract the top 20 labels\n",
    "for i in range(X_test_noHLC.shape[0]):\n",
    "    # Get the predicted probabilities for the i-th observation\n",
    "    proba = np.array([rf_proba_pred_noHLC[j][i, 0] for j in range(len(rf_proba_pred_noHLC))])\n",
    "    \n",
    "    # Get the indices of the top 20 labels sorted by probability\n",
    "    top_20_rf_indices_noHLC = np.argsort(proba)[:20][::-1]\n",
    "    \n",
    "    # Store the top 20 labels\n",
    "    top_20_indices_per_observation_rf_noHLC.append(top_20_rf_indices_noHLC)\n",
    "\n",
    "    top_20_rf_labels_noHLC = [concat_labels.columns[idx] for idx in top_20_rf_indices_noHLC]\n",
    "\n",
    "    print(top_20_rf_labels_noHLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_top_20_rf_noHLC = np.zeros((X_test_noHLC.shape[0], concat_labels.shape[1]), dtype=bool)\n",
    "for i, top_20_rf_indices_noHLC in enumerate(top_20_indices_per_observation_rf_noHLC):\n",
    "    y_pred_top_20_rf_noHLC[i, top_20_rf_indices_noHLC] = True\n",
    "\n",
    "# Calculate precision for the top 20 predicted labels\n",
    "precision = precision_score(y_test, y_pred_top_20_rf_noHLC, average='samples', zero_division=1)\n",
    "print(f\"Precision for top 20 labels: {precision}\") #0.5125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=0), rf_param_grid, cv=loo, scoring=f_scorer, n_jobs=-2)\n",
    "rf_grid_search.fit(X_train_HLC, y_train)\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(\"Best parameters:\", rf_grid_search.best_params_)\n",
    "print(\"Best score:\", rf_grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "rf_best_model_HLC = rf_grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels for the test set\n",
    "rf_y_pred_HLC = rf_best_model_HLC.predict(X_test_HLC)\n",
    "\n",
    "# Evaluate the model using precision, recall, and F2 score\n",
    "rf_metrics_HLC = precision_recall_fscore_support(y_test, rf_y_pred_HLC, average='macro', zero_division=1, beta=1)\n",
    "print(\"Recall on test set:\", rf_metrics_HLC[1])  # 0.550\n",
    "print(\"Precision on test set:\", rf_metrics_HLC[0])  # 0.952\n",
    "print(\"F2 Score on test set:\", rf_metrics_HLC[2])  # 0.516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting probability of labels\n",
    "rf_proba_pred_HLC = rf_best_model_HLC.predict_proba(X_test_HLC)\n",
    "\n",
    "# Initialize a list to store the top 20 labels for each observation\n",
    "top_20_indices_per_observation_rf_HLC = []\n",
    "\n",
    "# For each test observation, extract the top 20 labels\n",
    "for i in range(X_test_HLC.shape[0]):\n",
    "    # Get the predicted probabilities for the i-th observation\n",
    "    proba = np.array([rf_proba_pred_HLC[j][i, 0] for j in range(len(rf_proba_pred_HLC))])\n",
    "    \n",
    "    # Get the indices of the top 20 labels sorted by probability\n",
    "    top_20_rf_indices_HLC = np.argsort(proba)[:20][::-1]\n",
    "    \n",
    "    # Store the top 20 labels\n",
    "    top_20_indices_per_observation_rf_HLC.append(top_20_rf_indices_HLC)\n",
    "\n",
    "    top_20_rf_labels_HLC = [concat_labels.columns[idx] for idx in top_20_rf_indices_HLC]\n",
    "\n",
    "    print(top_20_rf_labels_HLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_top_20_rf_HLC = np.zeros((X_test_HLC.shape[0], concat_labels.shape[1]), dtype=bool)\n",
    "for i, top_20_rf_indices_HLC in enumerate(top_20_indices_per_observation_rf_HLC):\n",
    "    y_pred_top_20_rf_HLC[i, top_20_rf_indices_HLC] = True\n",
    "\n",
    "# Calculate precision for the top 20 predicted labels\n",
    "precision = precision_score(y_test, y_pred_top_20_rf_HLC, average='samples', zero_division=1)\n",
    "print(f\"Precision for top 20 labels: {precision}\") #0.5125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most important features\n",
    "# Get feature importances\n",
    "rf_feature_importances = rf_best_model_HLC.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importances\n",
    "rf_feature_importances_df = pd.DataFrame({'Feature': X_train_HLC.columns, 'Importance': rf_feature_importances})\n",
    "rf_feature_importances_df = rf_feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the top 10 most important features\n",
    "print(\"Top 10 most important features:\")\n",
    "print(rf_feature_importances_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run decision tree with all X\n",
    "scaler = StandardScaler()\n",
    "X_full = scaler.fit_transform(X_transformed[numerical_features])\n",
    "X_full_numeric = pd.DataFrame(X_full, columns=numerical_features, index=X_transformed.index)\n",
    "X_final = pd.concat([X_full_numeric, X_dummies], axis=1)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "dt_grid_search_final = GridSearchCV(DecisionTreeClassifier(random_state=0), dt_param_grid, cv=loo, scoring=f_scorer, n_jobs=-2)\n",
    "dt_grid_search_final.fit(X_final, y)\n",
    "\n",
    "# select the best model\n",
    "dt_best_final_model = dt_grid_search_final.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the best model in a pickle file\n",
    "import pickle\n",
    "\n",
    "# Save the best model to a file\n",
    "with open('best_final_model.pkl', 'wb') as file:\n",
    "    pickle.dump(dt_best_final_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
