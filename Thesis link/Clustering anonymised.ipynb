{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping Data for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.sparse import hstack\n",
    "from joblib import parallel_backend\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from pattern.nl import lemma\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all datasets seperately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for those structured visually not practically need to edit slightly because the use of merging leaves a lot of empty fields\n",
    "\n",
    "#first remove all the rows which do not reference a maatregel or a project\n",
    "Project_A = Project_A[(Project_A['Beheersmaatregel'].notna()) | (Project_A['Gebeurtenis'].notna())].reset_index(drop=True)\n",
    "\n",
    "#now make sure the project info is there for all rows which have a maatregel\n",
    "for i in range(len(Project_A['Risico code'])):\n",
    "    if pd.isnull(Project_A.loc[i,'Risico code']):\n",
    "        Project_A.iloc[i, 0:17] = Project_A.iloc[i-1,0:17]\n",
    "        Project_A.iloc[i, 21:] = Project_A.iloc[i-1,21:] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of things to vectorise on\n",
    "- name + description\n",
    "- Cause and effect + Measures taken and Measure type\n",
    "- Category + Allocatie "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing standard column names and converting everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []\n",
    "names = []\n",
    "for (key, value) in globals().copy().items():\n",
    "    if isinstance(value, pd.DataFrame):\n",
    "        projects.append(value)\n",
    "        names.append(key)\n",
    "\n",
    "wanted_columns = ['Naam', 'Omschrijving', 'Oorzaak', 'Gevolg', 'Maatregel', 'Type Maatregel', 'Categorie', 'Allocatie', 'Kans I', 'Tijd I', 'Geld I', 'Project']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I need to add the name of the dataframe as the last columns for each dataframe\n",
    "for i in range(len(projects)):\n",
    "    projects[i]['Project'] = names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making column names consistent\n",
    "\n",
    "for i in range(len(projects)):\n",
    "    for col_name in wanted_columns:\n",
    "        if col_name not in projects[i].columns:\n",
    "            print(names[i], col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns which had wrong names. I had 27 datasets which had up to four wrong column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some projects dont have certain columns but we add blank ones to not cause errors\n",
    "Project_A['Maatregel'] = np.nan\n",
    "Project_A['Type Maatregel'] = np.nan\n",
    "Project_B['Omschrijving'] = np.nan\n",
    "Project_B['Type Maatregel'] = np.nan\n",
    "Project_B['Categorie'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check column names\n",
    "for i in range(len(projects)):\n",
    "    for col_name in wanted_columns:\n",
    "        if col_name not in projects[i].columns:\n",
    "            print(names[i], col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine with another dataset\n",
    "combined_data = pd.concat((project for project in projects), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning NaNs into a textual value because an NaN can also have a meaning and dropping not neeeded columns\n",
    "for i in wanted_columns:\n",
    "    combined_data[i].fillna('None', inplace=True)\n",
    "\n",
    "combined_data = combined_data[wanted_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the values within the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure all factors are the same\n",
    "print(combined_data['Type Maatregel'].unique())\n",
    "print(combined_data['Categorie'].unique())\n",
    "print(len(combined_data['Allocatie'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first one seems fine\n",
    "\n",
    "#second one required changes\n",
    "combined_data['Categorie'].replace(to_replace= ['Techniek', 'Veiligheid', 'Tijd&Geld', 'Omgeving', 'Organisatie', 'Kwaliteit', 'Vergunningen', 'Category RIS', 'Contract', 'Mileu', 'Uitvoering', 'Ontwerp', 'Vergunning', 'Planning', 'K&L',\n",
    "                                                \"SLOT's\", 'Proces', 'Wetgeving', 'GWW', 'Civiel', 'Opdrachtgever', 'Verkeersmaatregelen', 'Planning en fasering', 'Contractueel', 'Technisch Realisatie', 'Technisch Ontwerp', 'Beheer en onderhoud',\n",
    "                                                'Geografisch', 'Project Afronding Beheersing', 'Werkvoorbereiding', 'Realisatie', 'Politiek / bestuurlijk', 'Milieu'], \n",
    "                                   value=['Technisch', 'Ruimtelijk', 'Financieel', 'Ruimtelijk', 'Organisatorisch', 'Ruimtelijk', 'Politiek', 'Juridisch', 'Juridisch', 'Ruimtelijk', 'Organisatorisch', 'Technisch', 'Politiek', 'Organisatorisch', 'Organisatorisch',\n",
    "                                          'Ruimtelijk', 'Organisatorisch', 'Juridisch', 'Technisch', 'Technisch' , 'Organisatorisch', 'Maatschappelijk', 'Organisatorisch', 'Juridisch', 'Technisch', 'Technisch', 'Technisch',\n",
    "                                           'Ruimtelijk', 'Organisatorisch', 'Organisatorisch', 'Technisch', 'Politiek', 'Ruimtelijk' ], inplace=True)\n",
    "\n",
    "#third requires only small change\n",
    "combined_data['Allocatie'].replace(to_replace= ['OG/ON', 'ON / OG', 'ON2', 'OG3', 'ON1', 'OG4', 'OG/ ON', 'OG (met impact ON)', 'ON (met impact OG)', 'OG / ON'],\n",
    "                                   value=['ON/OG', 'ON/OG', 'ON', 'OG', 'ON', 'OG', 'ON/OG', 'ON/OG', 'ON/OG', 'ON/OG'], inplace=True)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following need to be converted to numerical factors\n",
    "print(combined_data['Kans I'].unique())\n",
    "print(combined_data['Tijd I'].unique())\n",
    "print(combined_data['Geld I'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets start converting\n",
    "#dijk zwolle nog niet want 7 niet 5. Moerdijk 6\n",
    "combined_data['Kans I'].replace(to_replace= ['Zeer onwaarschijnlijk', 'Kleine kans', 'Kans bestaat, niet groot', 'Reële kans', 'Grote kans', 'None',\n",
    "                                             'Zeer onwaarschijnlijk (RE)', 'Kleine kans (RE)', 'Kans bestaat, niet groot (RE)', 'Reële kans (RE)', 'Grote kans (RE)', 'Zeer grote kans',\n",
    "                                             'Onwaarschijnlijk', 'Komt zelden voor', 'Zeer onwaarschijnlijk', 'Kleine kans', 'Geen', 'Zeer groot', 'Zeker', 'Onzekerheid',\n",
    "                                             'Zeer klein (0-5%)', 'Klein (5-13%)', 'Redelijk (13-25%)', 'Groot (25-50%)', 'Zeer groot (50-100%)', 'Geen (0%)',\n",
    "                                             'Vrijwel zeker', 'Groot', 'Geringe kans (0-5%)', 'Kleine kans (5-10%)', \"Redelijke kans (10-25%)\", \"Grote kans (25-50%)\", \"Vrijwel zeker (50-100%)\",\n",
    "                                             'Kan niet optreden', '2) Onwaarschijnlijk', '3) Kans bestaat, niet groot', '4) Reële kans', 'Er is een reëele kans', 'Grote kans, waarschijnlijk', \n",
    "                                              '0-1%', '1-10%', '10-25%', '25-50%', '50-100%',\n",
    "                                              'Bijna onmogelijk (0-1%)', 'Onwaarschijnlijk (1-10%)', 'Reële kans (10-25%)', 'Grote kans (25-50%)', 'Grote kans (>50%)'],\n",
    "                                 value=[1, 2, 3, 4, 5, 0,\n",
    "                                        1, 2, 3, 4, 5, 5,\n",
    "                                        1, 2, 1, 2, 0, 5, 4, 2,\n",
    "                                        1, 2, 3, 4, 5, 0,\n",
    "                                        5, 4, 1, 2, 3, 4, 5,\n",
    "                                        0, 2, 3, 4, 4, 5,\n",
    "                                        1, 2, 3, 4, 5,\n",
    "                                        1, 2, 3, 4, 5], inplace=True)\n",
    "\n",
    "\n",
    "combined_data['Tijd I'].replace(to_replace= ['Zeer klein', 'Klein', 'Matig', 'Gemiddeld', 'Groot', 'Zeer groot', 'None',\n",
    "                                             'Zeer klein risico', 'Klein risico', 'Gemiddeld risico', 'Groot risico', 'Zeer groot risico', 'Geen',\n",
    "                                             'Zeer laag risico', 'Laag risico', 'Hoog risico', 'Extreem hoog risico', 'Extreem risico', 'Time', 'Extreem laag risico',\n",
    "                                             'Laag', 'Medium', 'Hoog', 'Extreem', 'Kleine kans', 'Gemiddelde kans', 'Grote kans', 'Zeer kleine kans', 'Ernstig', 'Kleine kans (- 1-2 weken)',\n",
    "                                             '< 1 mnd', '1 - 3 mnd', '3 - 6 mnd', '6 -12 mnd',\n",
    "                                              '0) Geen', '1) < 1 week', '2) 1 week - 1 maand', '3) 1 - 3 maanden', '4) > 3 maanden',\n",
    "                                              '0 tot 0 wkn' ,'0 tot 1 wkn', '1 tot 3 wkn', '3 tot 9 wkn', '9 tot 27 wkn', '27 tot 81 wkn',\n",
    "                                              '+ 0 − 1mnd', '+ 1 − 3mnd', '+ 3 − 6mnd', '+ 6 −12 mnd', '+ 6 − 12mnd', '0',\n",
    "                                              'Geen vertraging', 'Zeer klein risico (+ < 1 week)', 'Klein risico (+ 1- 2 weken)', 'Middelgroot risico (+ 2-4 weken)', 'Groot risico (+ 4-8 weken)', 'Zeer groot risico (+ >= 8 weken)'],\n",
    "                                 value=[1, 2, 3, 3, 4, 5, 0,\n",
    "                                        1, 2, 3, 4, 5, 0,\n",
    "                                        1, 2, 4, 5, 5, 0, 1,\n",
    "                                        1, 2, 3, 5, -2, -3, -4, -1, 4, -2,\n",
    "                                        1, 2, 3, 4,\n",
    "                                        0, 1, 2, 3, 4,\n",
    "                                        0, 1, 2, 3, 4, 5,\n",
    "                                        1, 2, 3, 4, 4, 0,\n",
    "                                        0, 1, 2, 3, 4, 5], inplace=True)\n",
    "\n",
    "combined_data['Geld I'].replace(to_replace= ['Zeer klein', 'Klein', 'Matig', 'Gemiddeld', 'Groot', 'Zeer groot', 'Zeer Groot', 'None',\n",
    "                                             'Zeer klein risico', 'Klein risico', 'klein risico', 'Gemiddeld risico', 'Groot risico', 'Zeer groot risico', 'Geen',\n",
    "                                             'Extreem laag risico', 'Laag risico', 'Hoog risico', 'Zeer hoog risico', 'Extreem risico', 'Heel hoog risico',\n",
    "                                             'Geen extra kosten', 'Lage extra kosten', 'Marginale extra kosten', 'Redelijke extra kosten', 'Hoge extra kosten', 'Zeer hoge extra kosten', 'Time',\n",
    "                                             'Laag', 'Medium', 'Hoog', 'Extreem', 'Kleine kans', 'Gemiddelde kans', 'Grote kans', 'Extreme kans',\n",
    "                                             '< 0.25 mln', '0.25-0.5 mln', '0.5-1 mln', '> 2 mln', 'Zeer kleine kans',\n",
    "                                             '0) Geen', '1) 0 - 185.000', '2) 185.000 - 925.000', '3) 925.000 - 4.625.000', '4) 4.625.000 - 13.875.000',\n",
    "                                             '€ 0 tot € 10.000', '€ 10.000 tot € 75.000', '€ 75.000 tot € 200.000', '€ 200.000 tot € 750.000', '€ 750.000 tot € 2.000.000',\n",
    "                                             '0 − 100k', '100k − 250k', '250k − 500k', '500k − 1mln', '1-2 mln', '0',\n",
    "                                             '€ 0 - € 100.000 (risico)', '€ 100.000 - € 250.000 (risico)', '€ 250.000 - € 750.000 (risico)', '€ 750.000 - € 1.500.000 (risico)', '> € 1.500.000 (risico)', '€ 250.000 - € 750.000 (kans)'],\n",
    "\n",
    "                                 value=[1, 2, 3, 3, 4, 5, 5, 0,\n",
    "                                        1, 2, 2, 3, 4, 5, 0,\n",
    "                                        1, 2, 4, 5, 5, 5,\n",
    "                                        0, 1, 2, 3, 4, 5, 0,\n",
    "                                        1, 2, 3, 5, -1, -2, -4, -5,\n",
    "                                        1, 2, 3, 5, -1,\n",
    "                                        0, 1, 2, 3, 4,\n",
    "                                        1, 2, 3, 4, 5,\n",
    "                                        1, 2, 3, 4, 5, 0,\n",
    "                                        1, 2, 3, 4, 5, -3], inplace=True)\n",
    "\n",
    "combined_data['Kans I'] = combined_data['Kans I'].astype(int)\n",
    "combined_data['Tijd I'] = combined_data['Tijd I'].astype(int)\n",
    "combined_data['Geld I'] = combined_data['Geld I'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dimensions\n",
    "\n",
    "combined_data['text_variables'] = combined_data['Naam'] + ' ' + combined_data['Omschrijving'] + ' ' + combined_data['Oorzaak'] + ' '+ combined_data['Gevolg'] + ' ' + combined_data['Maatregel'] + ' ' + combined_data['Type Maatregel']\n",
    "\n",
    "\n",
    "#checking for NaNs\n",
    "print(combined_data['text_variables'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates for dimension 1\n",
    "combined_data = combined_data[(combined_data['text_variables'].duplicated() == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['Project'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.sparse import hstack\n",
    "from joblib import parallel_backend\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from pattern.nl import lemma\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_excel(\"Info sheets\\\\input_sub_category_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates for dimension 1\n",
    "combined_data = combined_data[(combined_data['text_variables'].duplicated() == False)]\n",
    "\n",
    "#exclude status Concept and Risico of Kans Kans\n",
    "combined_data = combined_data[(combined_data['Risico of Kans'] != 'Kans')].reset_index(drop=True)\n",
    "\n",
    "#select wanted columns\n",
    "combined_data = combined_data[['Naam', 'Omschrijving', 'Oorzaak', 'Gevolg', 'Maatregel', 'Type Maatregel', 'Categorie', 'Subcategorie', 'Allocatie', 'Project', 'text_variables']]\n",
    "\n",
    "#remove values if they have missing values in more than 5 columns which are missing values\n",
    "combined_data = combined_data[(combined_data.isna().sum(axis=1) < 5)].reset_index(drop=True)\n",
    "\n",
    "#Turn Contract into Juridisch\n",
    "combined_data['Categorie'].replace(to_replace='Contract', value='Juridisch', inplace=True)\n",
    "\n",
    "#if the Subcategorie has a '? ' in it, we remove it\n",
    "combined_data['Subcategorie'] = combined_data['Subcategorie'].str.replace('? ', 'None')\n",
    "combined_data['Subcategorie'] = combined_data['Subcategorie'].str.replace('?', 'None')\n",
    "\n",
    "#turning NAs into 'None'\n",
    "for i in combined_data.columns:\n",
    "    combined_data[i].fillna('None', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(combined_data['Gevolg'] == 'None').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of None values in each column\n",
    "for i in combined_data.columns:\n",
    "    print(i, (combined_data[i] == 'None').sum()/len(combined_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot for Allocation\n",
    "plt.subplot(1, 2, 1)\n",
    "combined_data['Allocatie'].value_counts().plot(kind='bar', fontsize=15)\n",
    "plt.xlabel('Allocation', fontsize=15)\n",
    "plt.ylabel('Number of observations', fontsize=17)\n",
    "\n",
    "# Plot for Category\n",
    "plt.subplot(1, 2, 2)\n",
    "combined_data['Categorie'].value_counts()[:-1].plot(kind='bar', fontsize=15)\n",
    "plt.xlabel('Category', fontsize=17)\n",
    "\n",
    "\n",
    "# Adding (a) and (b) titles below the graphs\n",
    "plt.figtext(0.25, 0.01, '(a) Observations by allocation', ha='center', fontsize=20)\n",
    "plt.figtext(0.75, 0.01, '(b) Observations by category', ha='center', fontsize=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of observations in category None\n",
    "print(f\"Percentage of observations in category None: {round((combined_data['Categorie'].value_counts()['Technisch'] / len(combined_data)) * 100, 2)}%\")\n",
    "print(f\"Percentage of observations in category None: {round((combined_data['Categorie'].value_counts()['Financieel'] / len(combined_data)) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of unique subcategories for each category\n",
    "subcat_counts_per_category = combined_data.groupby('Categorie')['Subcategorie'].nunique().reset_index(name='subcat_counts')\n",
    "category_counts = combined_data.groupby(['Categorie', 'Subcategorie']).size().reset_index(name='counts')\n",
    "\n",
    "pivot_table = category_counts.pivot(index='Categorie', columns='Subcategorie', values='counts').fillna(0)\n",
    "\n",
    "# Plot the bar chart with ordered subcategories, without the legend, and indicating the number of subcategories per category\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "pivot_table.plot(kind='bar', stacked=True, ax=ax, colormap='tab20', legend=False)\n",
    "\n",
    "# Adding text annotations for the number of subcategories\n",
    "for idx, row in subcat_counts_per_category.iterrows():\n",
    "    ax.text(idx, pivot_table.loc[row['Categorie']].sum() + 1, f\"{row['subcat_counts']} subcategories\", \n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Set the labels and title\n",
    "ax.set_xlabel('Categorie')\n",
    "ax.set_ylabel('Number of Observations')\n",
    "ax.set_title('Number of Observations for Each Category and Subcategory (Ordered by Frequency)')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of subcategories per category\n",
    "\n",
    "# Counting the number of unique subcategories for each category\n",
    "combined_data.groupby('Categorie')['Subcategorie'].nunique().reset_index(name='subcat_counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of observations per subcategory\n",
    "combined_data['Subcategorie'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split according to categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['Categorie'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe one hot encode on sub category and allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the datasets according to categories\n",
    "Maatschappelijk_data = combined_data[combined_data['Categorie'] == 'Maatschappelijk']\n",
    "Organisatorisch_data = combined_data[combined_data['Categorie'] == 'Organisatorisch']\n",
    "Politiek_data = combined_data[combined_data['Categorie'] == 'Politiek']\n",
    "Ruimtelijk_data = combined_data[combined_data['Categorie'] == 'Ruimtelijk']\n",
    "Technisch_data = combined_data[combined_data['Categorie'] == 'Technisch']\n",
    "Financieel_data = combined_data[combined_data['Categorie'] == 'Financieel']\n",
    "Juridisch_data = combined_data[combined_data['Categorie'] == 'Juridisch']\n",
    "unknown_data = combined_data[combined_data['Categorie'] == 'None']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode the subcategorie and allocatie columns\n",
    "Maatschappelijk_data = pd.concat([Maatschappelijk_data, pd.get_dummies(Maatschappelijk_data['Subcategorie'], prefix='Subcategorie').astype(int)], axis=1)\n",
    "Maatschappelijk_data = pd.concat([Maatschappelijk_data, pd.get_dummies(Maatschappelijk_data['Allocatie'], prefix='Allocatie').astype(int)], axis=1)\n",
    "\n",
    "Organisatorisch_data = pd.concat([Organisatorisch_data, pd.get_dummies(Organisatorisch_data['Subcategorie'], prefix='Subcategorie').astype(int)], axis=1)\n",
    "Organisatorisch_data = pd.concat([Organisatorisch_data, pd.get_dummies(Organisatorisch_data['Allocatie'], prefix='Allocatie').astype(int)], axis=1)\n",
    "\n",
    "Politiek_data = pd.concat([Politiek_data, pd.get_dummies(Politiek_data['Subcategorie'], prefix='Subcategorie').astype(int)], axis=1)\n",
    "Politiek_data = pd.concat([Politiek_data, pd.get_dummies(Politiek_data['Allocatie'], prefix='Allocatie').astype(int)], axis=1)\n",
    "\n",
    "Ruimtelijk_data = pd.concat([Ruimtelijk_data, pd.get_dummies(Ruimtelijk_data['Subcategorie'], prefix='Subcategorie').astype(int)], axis=1)\n",
    "Ruimtelijk_data = pd.concat([Ruimtelijk_data, pd.get_dummies(Ruimtelijk_data['Allocatie'], prefix='Allocatie').astype(int)], axis=1)\n",
    "\n",
    "Technisch_data = pd.concat([Technisch_data, pd.get_dummies(Technisch_data['Subcategorie'], prefix='Subcategorie').astype(int)], axis=1)\n",
    "Technisch_data = pd.concat([Technisch_data, pd.get_dummies(Technisch_data['Allocatie'], prefix='Allocatie').astype(int)], axis=1)\n",
    "\n",
    "Financieel_data = pd.concat([Financieel_data, pd.get_dummies(Financieel_data['Subcategorie'], prefix='Subcategorie').astype(int)], axis=1)\n",
    "Financieel_data = pd.concat([Financieel_data, pd.get_dummies(Financieel_data['Allocatie'], prefix='Allocatie').astype(int)], axis=1)\n",
    "\n",
    "Juridisch_data = pd.concat([Juridisch_data, pd.get_dummies(Juridisch_data['Subcategorie'], prefix='Subcategorie').astype(int)], axis=1)\n",
    "Juridisch_data = pd.concat([Juridisch_data, pd.get_dummies(Juridisch_data['Allocatie'], prefix='Allocatie').astype(int)], axis=1)\n",
    "\n",
    "unknown_data = pd.concat([unknown_data, pd.get_dummies(unknown_data['Subcategorie'], prefix='Subcategorie').astype(int)], axis=1)\n",
    "unknown_data = pd.concat([unknown_data, pd.get_dummies(unknown_data['Allocatie'], prefix='Allocatie').astype(int)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maatschappelijk_data.iloc[:,11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting tokenizer and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import parallel_backend\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "\n",
    "def tokenize_and_lemmatize_dutch(text):\n",
    "    text = text.replace('&', 'en')  # Replace '&' with 'en'\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]  # Remove stopwords\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    tokens = [lemma(token) for token in tokens] # Lemmatize\n",
    "    return tokens\n",
    "\n",
    "hyperparameters_grid = {\n",
    "    'max_df': np.linspace(0.02, 0.15, 5),\n",
    "    'min_df': np.linspace(0.0, 0.015, 3),\n",
    "    'ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'norm': ['l2', 'l1', None],\n",
    "    'sublinear_tf': [True, False]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K means for every category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maatschappelijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maatschappelijk_data.iloc[:,11:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import parallel_backend\n",
    "best_score = 0\n",
    "best_hyperparameters = None\n",
    "best_cluster_labels = None\n",
    "\n",
    "knn_pca_maatschappelijk = Maatschappelijk_data\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Maatschappelijk_data['text_variables'])\n",
    "                    \n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    combined_matrix = hstack([tfidf_matrix, Maatschappelijk_data.iloc[:,11:].values])\n",
    "                    #apply PCA\n",
    "                    clustering_matrix = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "                    # Perform KMeans clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        for k in range(10, 40):\n",
    "                            kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "                            cluster_labels = kmeans.fit_predict(clustering_matrix)\n",
    "                            silhouette_avg = silhouette_score(clustering_matrix, cluster_labels)\n",
    "                            \n",
    "                            # Update the best score and hyperparameters if the current score is better\n",
    "                            if silhouette_avg > best_score:\n",
    "                                best_score = silhouette_avg\n",
    "                                best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                best_cluster_labels = cluster_labels\n",
    "                                best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "knn_pca_maatschappelijk['Cluster_Labels'] = best_cluster_labels\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "cluster_ids, cluster_sizes = pd.Series(best_cluster_labels).value_counts().index, pd.Series(best_cluster_labels).value_counts().values\n",
    "print(f\"Number of clusters: {len(cluster_ids)} and number of elements assigned to each cluster: {cluster_sizes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "politiek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca_politiek = Politiek_data\n",
    "\n",
    "best_score = 0\n",
    "best_hyperparameters = None\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Politiek_data['text_variables'])\n",
    "                    \n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    combined_matrix = hstack([tfidf_matrix, Politiek_data.iloc[:,11:].values])\n",
    "                    #apply PCA\n",
    "                    clustering_matrix = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "                    # Perform KMeans clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        for k in range(10, 40):\n",
    "                            kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "                            cluster_labels = kmeans.fit_predict(clustering_matrix)\n",
    "                            silhouette_avg = silhouette_score(clustering_matrix, cluster_labels)\n",
    "                            \n",
    "                            # Update the best score and hyperparameters if the current score is better\n",
    "                            if silhouette_avg > best_score:\n",
    "                                best_score = silhouette_avg\n",
    "                                best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                best_cluster_labels = cluster_labels\n",
    "                                best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "knn_pca_politiek['Cluster_Labels'] = best_cluster_labels\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "cluster_ids, cluster_sizes = pd.Series(best_cluster_labels).value_counts().index, pd.Series(best_cluster_labels).value_counts().values\n",
    "print(f\"Number of clusters: {len(cluster_ids)} and number of elements assigned to each cluster: {cluster_sizes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruimtelijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca_ruimtelijk = Ruimtelijk_data\n",
    "\n",
    "best_score = 0\n",
    "best_hyperparameters = None\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Ruimtelijk_data['text_variables'])\n",
    "                    \n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    combined_matrix = hstack([tfidf_matrix, Ruimtelijk_data.iloc[:,11:].values])\n",
    "                    #apply PCA\n",
    "                    clustering_matrix = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "                    # Perform KMeans clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        for k in range(10, 40):\n",
    "                            kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "                            cluster_labels = kmeans.fit_predict(clustering_matrix)\n",
    "                            silhouette_avg = silhouette_score(clustering_matrix, cluster_labels)\n",
    "                            \n",
    "                            # Update the best score and hyperparameters if the current score is better\n",
    "                            if silhouette_avg > best_score:\n",
    "                                best_score = silhouette_avg\n",
    "                                best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                best_cluster_labels = cluster_labels\n",
    "                                best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "knn_pca_ruimtelijk['Cluster_Labels'] = best_cluster_labels\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "cluster_ids, cluster_sizes = pd.Series(best_cluster_labels).value_counts().index, pd.Series(best_cluster_labels).value_counts().values\n",
    "print(f\"Number of clusters: {len(cluster_ids)} and number of elements assigned to each cluster: {cluster_sizes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organisatorisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca_organisatorisch = Organisatorisch_data\n",
    "\n",
    "best_score = 0\n",
    "best_hyperparameters = None\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Organisatorisch_data['text_variables'])\n",
    "                    \n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    combined_matrix = hstack([tfidf_matrix, Organisatorisch_data.iloc[:,11:].values])\n",
    "                    #apply PCA\n",
    "                    clustering_matrix = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "                    # Perform KMeans clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        for k in range(10, 40):\n",
    "                            kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "                            cluster_labels = kmeans.fit_predict(clustering_matrix)\n",
    "                            silhouette_avg = silhouette_score(clustering_matrix, cluster_labels)\n",
    "                            \n",
    "                            # Update the best score and hyperparameters if the current score is better\n",
    "                            if silhouette_avg > best_score:\n",
    "                                best_score = silhouette_avg\n",
    "                                best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                best_cluster_labels = cluster_labels\n",
    "                                best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "knn_pca_organisatorisch['Cluster_Labels'] = best_cluster_labels\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "cluster_ids, cluster_sizes = pd.Series(best_cluster_labels).value_counts().index, pd.Series(best_cluster_labels).value_counts().values\n",
    "print(f\"Number of clusters: {len(cluster_ids)} and number of elements assigned to each cluster: {cluster_sizes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juridisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca_juridisch = Juridisch_data\n",
    "\n",
    "best_score = 0\n",
    "best_hyperparameters = None\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Juridisch_data['text_variables'])\n",
    "                    \n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    combined_matrix = hstack([tfidf_matrix, Juridisch_data.iloc[:,11:].values])\n",
    "                    #apply PCA\n",
    "                    clustering_matrix = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "                    # Perform KMeans clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        for k in range(10, 40):\n",
    "                            kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "                            cluster_labels = kmeans.fit_predict(clustering_matrix)\n",
    "                            silhouette_avg = silhouette_score(clustering_matrix, cluster_labels)\n",
    "                            \n",
    "                            # Update the best score and hyperparameters if the current score is better\n",
    "                            if silhouette_avg > best_score:\n",
    "                                best_score = silhouette_avg\n",
    "                                best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                best_cluster_labels = cluster_labels\n",
    "                                best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "knn_pca_juridisch['Cluster_Labels'] = best_cluster_labels\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "cluster_ids, cluster_sizes = pd.Series(best_cluster_labels).value_counts().index, pd.Series(best_cluster_labels).value_counts().values\n",
    "print(f\"Number of clusters: {len(cluster_ids)} and number of elements assigned to each cluster: {cluster_sizes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca_technisch = Technisch_data\n",
    "\n",
    "best_score = 0\n",
    "best_hyperparameters = None\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Technisch_data['text_variables'])\n",
    "                    \n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    combined_matrix = hstack([tfidf_matrix, Technisch_data.iloc[:,11:].values])\n",
    "                    #apply PCA\n",
    "                    clustering_matrix = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "                    # Perform KMeans clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        for k in range(10, 40):\n",
    "                            kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "                            cluster_labels = kmeans.fit_predict(clustering_matrix)\n",
    "                            silhouette_avg = silhouette_score(clustering_matrix, cluster_labels)\n",
    "                            \n",
    "                            # Update the best score and hyperparameters if the current score is better\n",
    "                            if silhouette_avg > best_score:\n",
    "                                best_score = silhouette_avg\n",
    "                                best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                best_cluster_labels = cluster_labels\n",
    "                                best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "knn_pca_technisch['Cluster_Labels'] = best_cluster_labels\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "cluster_ids, cluster_sizes = pd.Series(best_cluster_labels).value_counts().index, pd.Series(best_cluster_labels).value_counts().values\n",
    "print(f\"Number of clusters: {len(cluster_ids)} and number of elements assigned to each cluster: {cluster_sizes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financieel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca_financieel = Financieel_data\n",
    "\n",
    "best_score = 0\n",
    "best_hyperparameters = None\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Financieel_data['text_variables'])\n",
    "                    \n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    combined_matrix = hstack([tfidf_matrix, Financieel_data.iloc[:,11:].values])\n",
    "                    #apply PCA\n",
    "                    clustering_matrix = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "                    # Perform KMeans clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        for k in range(10, 40):\n",
    "                            kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "                            cluster_labels = kmeans.fit_predict(clustering_matrix)\n",
    "                            silhouette_avg = silhouette_score(clustering_matrix, cluster_labels)\n",
    "                            \n",
    "                            # Update the best score and hyperparameters if the current score is better\n",
    "                            if silhouette_avg > best_score:\n",
    "                                best_score = silhouette_avg\n",
    "                                best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                best_cluster_labels = cluster_labels\n",
    "                                best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "knn_pca_financieel['Cluster_Labels'] = best_cluster_labels\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "cluster_ids, cluster_sizes = pd.Series(best_cluster_labels).value_counts().index, pd.Series(best_cluster_labels).value_counts().values\n",
    "print(f\"Number of clusters: {len(cluster_ids)} and number of elements assigned to each cluster: {cluster_sizes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca_unknown = unknown_data\n",
    "\n",
    "best_score = 0\n",
    "best_hyperparameters = None\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(unknown_data['text_variables'])\n",
    "                    \n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    combined_matrix = hstack([tfidf_matrix, unknown_data.iloc[:,11:].values])\n",
    "                    #apply PCA\n",
    "                    clustering_matrix = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "                    # Perform KMeans clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        for k in range(10, 40):\n",
    "                            kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "                            cluster_labels = kmeans.fit_predict(clustering_matrix)\n",
    "                            silhouette_avg = silhouette_score(clustering_matrix, cluster_labels)\n",
    "                            \n",
    "                            # Update the best score and hyperparameters if the current score is better\n",
    "                            if silhouette_avg > best_score:\n",
    "                                best_score = silhouette_avg\n",
    "                                best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                best_cluster_labels = cluster_labels\n",
    "                                best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "knn_pca_unknown['Cluster_Labels'] = best_cluster_labels\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "cluster_ids, cluster_sizes = pd.Series(best_cluster_labels).value_counts().index, pd.Series(best_cluster_labels).value_counts().values\n",
    "print(f\"Number of clusters: {len(cluster_ids)} and number of elements assigned to each cluster: {cluster_sizes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each new dataframe, give a prefix to the cluster labels\n",
    "knn_pca_maatschappelijk['Cluster_Labels'] = 'Maatschappelijk_' + knn_pca_maatschappelijk['Cluster_Labels'].astype(str)\n",
    "knn_pca_politiek['Cluster_Labels'] = 'Politiek_' + knn_pca_politiek['Cluster_Labels'].astype(str)\n",
    "knn_pca_ruimtelijk['Cluster_Labels'] = 'Ruimtelijk_' + knn_pca_ruimtelijk['Cluster_Labels'].astype(str)\n",
    "knn_pca_organisatorisch['Cluster_Labels'] = 'Organisatorisch_' + knn_pca_organisatorisch['Cluster_Labels'].astype(str)\n",
    "knn_pca_juridisch['Cluster_Labels'] = 'Juridisch_' + knn_pca_juridisch['Cluster_Labels'].astype(str)\n",
    "knn_pca_technisch['Cluster_Labels'] = 'Technisch_' + knn_pca_technisch['Cluster_Labels'].astype(str)\n",
    "knn_pca_financieel['Cluster_Labels'] = 'Financieel_' + knn_pca_financieel['Cluster_Labels'].astype(str)\n",
    "knn_pca_unknown['Cluster_Labels'] = 'Unknown_' + knn_pca_unknown['Cluster_Labels'].astype(str)\n",
    "\n",
    "#vertically stack all of them\n",
    "knn_all = pd.concat([knn_pca_maatschappelijk, knn_pca_politiek, knn_pca_ruimtelijk, knn_pca_organisatorisch, knn_pca_juridisch, knn_pca_technisch, knn_pca_financieel, knn_pca_unknown], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "knn_all.to_excel('Info sheets\\\\knn_all.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "\n",
    "def tokenize_and_lemmatize_dutch(text):\n",
    "    text = text.replace('&', 'en')  # Replace '&' with 'en'\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]  # Remove stopwords\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    tokens = [lemma(token) for token in tokens] # Lemmatize\n",
    "    return tokens\n",
    "\n",
    "hyperparameters_grid = {\n",
    "    'max_df': np.linspace(0.02, 0.15, 5),\n",
    "    'min_df': np.linspace(0.0, 0.015, 3),\n",
    "    'ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'norm': ['l2', 'l1', None],\n",
    "    'sublinear_tf': [True, False]\n",
    "}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Define a range of eps and min_samples values to try\n",
    "eps_range = np.linspace(0.1, 2, 10) \n",
    "min_samples_range = [1, 2, 3, 5, 7, 10]  # Adjust the range as needed\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maatschappelijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca_maatschappelijk = Maatschappelijk_data\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Maatschappelijk_data['text_variables'])\n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    clustering_matrix = hstack((tfidf_matrix, Maatschappelijk_data.iloc[:,11:-2].values))\n",
    "                    # Apply PCA to reduce dimensionality for visualization\n",
    "                    reduced_data = pca.fit_transform(clustering_matrix)\n",
    "                    # Perform DBSCAN clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        # Iterate over each combination of eps and min_samples\n",
    "                        for eps in eps_range:\n",
    "                            for min_samples in min_samples_range:\n",
    "                                # Perform DBSCAN clustering\n",
    "                                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                                cluster_labels = dbscan.fit_predict(reduced_data)\n",
    "                                \n",
    "                                # Check if any clusters were formed\n",
    "                                if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette_score\n",
    "                                    # Evaluate clustering performance using silhouette score\n",
    "                                    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "                                    \n",
    "                                    # Update the best score and parameters if the current score is better\n",
    "                                    if silhouette_avg > best_score:\n",
    "                                        best_score = silhouette_avg\n",
    "                                        best_eps = eps\n",
    "                                        best_min_samples = min_samples\n",
    "                                        best_cluster_labels = cluster_labels\n",
    "                                        best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                        best_pca = reduced_data\n",
    "\n",
    "\n",
    "# Print the best parameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Maatschappelijk_\"\n",
    "db_pca_maatschappelijk['DBSCAN_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Politiek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca_politiek = Politiek_data\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Politiek_data['text_variables'])\n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    clustering_matrix = hstack((tfidf_matrix, Politiek_data.iloc[:,11:-2].values))\n",
    "                    # Apply PCA to reduce dimensionality for visualization\n",
    "                    reduced_data = pca.fit_transform(clustering_matrix)\n",
    "                    # Perform DBSCAN clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        # Iterate over each combination of eps and min_samples\n",
    "                        for eps in eps_range:\n",
    "                            for min_samples in min_samples_range:\n",
    "                                # Perform DBSCAN clustering\n",
    "                                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                                cluster_labels = dbscan.fit_predict(reduced_data)\n",
    "                                \n",
    "                                # Check if any clusters were formed\n",
    "                                if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette_score\n",
    "                                    # Evaluate clustering performance using silhouette score\n",
    "                                    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "                                    \n",
    "                                    # Update the best score and parameters if the current score is better\n",
    "                                    if silhouette_avg > best_score:\n",
    "                                        best_score = silhouette_avg\n",
    "                                        best_eps = eps\n",
    "                                        best_min_samples = min_samples\n",
    "                                        best_cluster_labels = cluster_labels\n",
    "                                        best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                        best_pca = reduced_data\n",
    "\n",
    "\n",
    "# Print the best parameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Politiek_\"\n",
    "db_pca_politiek['DBSCAN_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruimtelijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca_ruimtelijk = Ruimtelijk_data\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Ruimtelijk_data['text_variables'])\n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    clustering_matrix = hstack((tfidf_matrix, Ruimtelijk_data.iloc[:,11:-2].values))\n",
    "                    # Apply PCA to reduce dimensionality for visualization\n",
    "                    reduced_data = pca.fit_transform(clustering_matrix)\n",
    "                    # Perform DBSCAN clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        # Iterate over each combination of eps and min_samples\n",
    "                        for eps in eps_range:\n",
    "                            for min_samples in min_samples_range:\n",
    "                                # Perform DBSCAN clustering\n",
    "                                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                                cluster_labels = dbscan.fit_predict(reduced_data)\n",
    "                                \n",
    "                                # Check if any clusters were formed\n",
    "                                if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette_score\n",
    "                                    # Evaluate clustering performance using silhouette score\n",
    "                                    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "                                    \n",
    "                                    # Update the best score and parameters if the current score is better\n",
    "                                    if silhouette_avg > best_score:\n",
    "                                        best_score = silhouette_avg\n",
    "                                        best_eps = eps\n",
    "                                        best_min_samples = min_samples\n",
    "                                        best_cluster_labels = cluster_labels\n",
    "                                        best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                        best_pca = reduced_data\n",
    "\n",
    "\n",
    "# Print the best parameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Ruimtelijk_\"\n",
    "db_pca_ruimtelijk['DBSCAN_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organisatorisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca_organisatorisch = Organisatorisch_data\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Organisatorisch_data['text_variables'])\n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    clustering_matrix = hstack((tfidf_matrix, Organisatorisch_data.iloc[:,11:-2].values))\n",
    "                    # Apply PCA to reduce dimensionality for visualization\n",
    "                    reduced_data = pca.fit_transform(clustering_matrix)\n",
    "                    # Perform DBSCAN clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        # Iterate over each combination of eps and min_samples\n",
    "                        for eps in eps_range:\n",
    "                            for min_samples in min_samples_range:\n",
    "                                # Perform DBSCAN clustering\n",
    "                                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                                cluster_labels = dbscan.fit_predict(reduced_data)\n",
    "                                \n",
    "                                # Check if any clusters were formed\n",
    "                                if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette_score\n",
    "                                    # Evaluate clustering performance using silhouette score\n",
    "                                    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "                                    \n",
    "                                    # Update the best score and parameters if the current score is better\n",
    "                                    if silhouette_avg > best_score:\n",
    "                                        best_score = silhouette_avg\n",
    "                                        best_eps = eps\n",
    "                                        best_min_samples = min_samples\n",
    "                                        best_cluster_labels = cluster_labels\n",
    "                                        best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                        best_pca = reduced_data\n",
    "\n",
    "\n",
    "# Print the best parameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Organisatorisch_\"\n",
    "db_pca_organisatorisch['DBSCAN_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juridisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca_juridisch = Juridisch_data\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Juridisch_data['text_variables'])\n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    clustering_matrix = hstack((tfidf_matrix, Juridisch_data.iloc[:,11:-2].values))\n",
    "                    # Apply PCA to reduce dimensionality for visualization\n",
    "                    reduced_data = pca.fit_transform(clustering_matrix)\n",
    "                    # Perform DBSCAN clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        # Iterate over each combination of eps and min_samples\n",
    "                        for eps in eps_range:\n",
    "                            for min_samples in min_samples_range:\n",
    "                                # Perform DBSCAN clustering\n",
    "                                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                                cluster_labels = dbscan.fit_predict(reduced_data)\n",
    "                                \n",
    "                                # Check if any clusters were formed\n",
    "                                if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette_score\n",
    "                                    # Evaluate clustering performance using silhouette score\n",
    "                                    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "                                    \n",
    "                                    # Update the best score and parameters if the current score is better\n",
    "                                    if silhouette_avg > best_score:\n",
    "                                        best_score = silhouette_avg\n",
    "                                        best_eps = eps\n",
    "                                        best_min_samples = min_samples\n",
    "                                        best_cluster_labels = cluster_labels\n",
    "                                        best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                        best_pca = reduced_data\n",
    "\n",
    "\n",
    "# Print the best parameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Juridisch_\"\n",
    "db_pca_juridisch['DBSCAN_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca_technisch = Technisch_data\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Technisch_data['text_variables'])\n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    clustering_matrix = hstack((tfidf_matrix, Technisch_data.iloc[:,11:-2].values))\n",
    "                    # Apply PCA to reduce dimensionality for visualization\n",
    "                    reduced_data = pca.fit_transform(clustering_matrix)\n",
    "                    # Perform DBSCAN clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        # Iterate over each combination of eps and min_samples\n",
    "                        for eps in eps_range:\n",
    "                            for min_samples in min_samples_range:\n",
    "                                # Perform DBSCAN clustering\n",
    "                                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                                cluster_labels = dbscan.fit_predict(reduced_data)\n",
    "                                \n",
    "                                # Check if any clusters were formed\n",
    "                                if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette_score\n",
    "                                    # Evaluate clustering performance using silhouette score\n",
    "                                    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "                                    \n",
    "                                    # Update the best score and parameters if the current score is better\n",
    "                                    if silhouette_avg > best_score:\n",
    "                                        best_score = silhouette_avg\n",
    "                                        best_eps = eps\n",
    "                                        best_min_samples = min_samples\n",
    "                                        best_cluster_labels = cluster_labels\n",
    "                                        best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                        best_pca = reduced_data\n",
    "\n",
    "\n",
    "# Print the best parameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Technisch_\"\n",
    "db_pca_technisch['DBSCAN_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financieel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca_financieel = Financieel_data\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(Financieel_data['text_variables'])\n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    clustering_matrix = hstack((tfidf_matrix, Financieel_data.iloc[:,11:-2].values))\n",
    "                    # Apply PCA to reduce dimensionality for visualization\n",
    "                    reduced_data = pca.fit_transform(clustering_matrix)\n",
    "                    # Perform DBSCAN clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        # Iterate over each combination of eps and min_samples\n",
    "                        for eps in eps_range:\n",
    "                            for min_samples in min_samples_range:\n",
    "                                # Perform DBSCAN clustering\n",
    "                                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                                cluster_labels = dbscan.fit_predict(reduced_data)\n",
    "                                \n",
    "                                # Check if any clusters were formed\n",
    "                                if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette_score\n",
    "                                    # Evaluate clustering performance using silhouette score\n",
    "                                    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "                                    \n",
    "                                    # Update the best score and parameters if the current score is better\n",
    "                                    if silhouette_avg > best_score:\n",
    "                                        best_score = silhouette_avg\n",
    "                                        best_eps = eps\n",
    "                                        best_min_samples = min_samples\n",
    "                                        best_cluster_labels = cluster_labels\n",
    "                                        best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                        best_pca = reduced_data\n",
    "\n",
    "\n",
    "# Print the best parameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Financieel_\"\n",
    "db_pca_financieel['DBSCAN_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca_unknown = unknown_data\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for max_df in hyperparameters_grid['max_df']:\n",
    "    for min_df in hyperparameters_grid['min_df']:\n",
    "        for ngram_range in hyperparameters_grid['ngram_range']:\n",
    "            for norm in hyperparameters_grid['norm']:\n",
    "                for sublinear_tf in hyperparameters_grid['sublinear_tf']:\n",
    "                    # Initialize TfidfVectorizer\n",
    "                    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize_dutch, ngram_range=ngram_range, max_df=max_df, min_df=min_df, norm=norm, sublinear_tf=sublinear_tf)\n",
    "                    # Fit and transform the text data\n",
    "                    tfidf_matrix = tfidf_vectorizer.fit_transform(unknown_data['text_variables'])\n",
    "                    # Combine the TF-IDF matrix with the dummy variables created fro category and allocatie\n",
    "                    clustering_matrix = hstack((tfidf_matrix, unknown_data.iloc[:,11:-2].values))\n",
    "                    # Apply PCA to reduce dimensionality for visualization\n",
    "                    reduced_data = pca.fit_transform(clustering_matrix)\n",
    "                    # Perform DBSCAN clustering with parallel processing\n",
    "                    with parallel_backend('threading', n_jobs=-2):\n",
    "                        # Iterate over each combination of eps and min_samples\n",
    "                        for eps in eps_range:\n",
    "                            for min_samples in min_samples_range:\n",
    "                                # Perform DBSCAN clustering\n",
    "                                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                                cluster_labels = dbscan.fit_predict(reduced_data)\n",
    "                                \n",
    "                                # Check if any clusters were formed\n",
    "                                if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette_score\n",
    "                                    # Evaluate clustering performance using silhouette score\n",
    "                                    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "                                    \n",
    "                                    # Update the best score and parameters if the current score is better\n",
    "                                    if silhouette_avg > best_score:\n",
    "                                        best_score = silhouette_avg\n",
    "                                        best_eps = eps\n",
    "                                        best_min_samples = min_samples\n",
    "                                        best_cluster_labels = cluster_labels\n",
    "                                        best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "                                        best_pca = reduced_data\n",
    "\n",
    "\n",
    "# Print the best parameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"unknown_\"\n",
    "db_pca_unknown['DBSCAN_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all the dataframes\n",
    "db_all = pd.concat([db_pca_maatschappelijk, db_pca_politiek, db_pca_ruimtelijk, db_pca_organisatorisch, db_pca_juridisch, db_pca_technisch, db_pca_financieel, db_pca_unknown], axis=0)\n",
    "\n",
    "#save the dataframe\n",
    "db_all.to_excel('Info sheets\\\\db_all.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance from the origin for each point\n",
    "distances = np.linalg.norm(best_pca, axis=1)\n",
    "\n",
    "# Identify the 8 points that are farthest from the origin\n",
    "outlier_indices = np.argsort(distances)[-13:]\n",
    "\n",
    "# Filter out the outliers\n",
    "filtered_indices = np.setdiff1d(np.arange(best_pca.shape[0]), outlier_indices)\n",
    "filtered_data = reduced_data[filtered_indices]\n",
    "filtered_labels = np.array(best_cluster_labels)[filtered_indices]\n",
    "\n",
    "\n",
    "# Plot the filtered data\n",
    "plt.figure(figsize=(10, 6))\n",
    "unique_labels = np.unique(filtered_labels)\n",
    "colors = plt.cm.get_cmap('viridis', len(unique_labels))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    plt.scatter(filtered_data[filtered_labels == label, 0], \n",
    "                filtered_data[filtered_labels == label, 1], \n",
    "                c=[colors(i)], label=f\"Cluster {label}\")\n",
    "\n",
    "plt.title('PCA Visualization of Clusters (Filtered)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data by cluster\n",
    "#combined_data.sort_values(by='Cluster_Labels', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!C:\\Users\\leen\\AppData\\Local\\anaconda3\\python.exe -m pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maatschappelijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pca_db_maatschappelijk = Maatschappelijk_data.copy()\n",
    "\n",
    "# Generate sentence embeddings for the text variables\n",
    "sentence_embeddings_maatschappelijk = model.encode(Maatschappelijk_data['text_variables'].tolist())\n",
    "\n",
    "# Combine the sentence embeddings with the other features (assuming they start from column 11)\n",
    "clustering_matrix = np.concatenate((sentence_embeddings_maatschappelijk, Maatschappelijk_data.iloc[:,11:-1].values), axis=1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "reduced_data = pca.fit_transform(clustering_matrix)\n",
    "\n",
    "best_k = None\n",
    "best_hyperparameters = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-2):\n",
    "    for k in range(10, 40):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        \n",
    "        # Update the best score and hyperparameters if the current score is better\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Maatschappelijk_\"\n",
    "embed_pca_db_maatschappelijk['embed_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot reduced data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(reduced_data[:,0], reduced_data[:,1])\n",
    "\n",
    "plt.title('PCA Visualization of Clusters (Filtered)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Politiek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pca_db_politiek = Politiek_data.copy()    \n",
    "\n",
    "# Generate sentence embeddings for the text variables\n",
    "sentence_embeddings_politiek = model.encode(Politiek_data['text_variables'].tolist())\n",
    "\n",
    "# Combine the sentence embeddings with the other features (assuming they start from column 11)\n",
    "clustering_matrix = np.concatenate((sentence_embeddings_politiek, Politiek_data.iloc[:,11:-1].values), axis=1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "reduced_data = pca.fit_transform(clustering_matrix)\n",
    "\n",
    "best_k = None\n",
    "best_hyperparameters = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-2):\n",
    "    for k in range(10, 40):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        \n",
    "        # Update the best score and hyperparameters if the current score is better\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Politiek_\"\n",
    "embed_pca_db_politiek['embed_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruimtelijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pca_db_ruimtelijk = Ruimtelijk_data.copy()\n",
    "\n",
    "# Generate sentence embeddings for the text variables\n",
    "sentence_embeddings_ruimtelijk = model.encode(Ruimtelijk_data['text_variables'].tolist())\n",
    "\n",
    "# Combine the sentence embeddings with the other features (assuming they start from column 11)\n",
    "clustering_matrix = np.concatenate((sentence_embeddings_ruimtelijk, Ruimtelijk_data.iloc[:,11:-1].values), axis=1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "reduced_data = pca.fit_transform(clustering_matrix)\n",
    "\n",
    "best_k = None\n",
    "best_hyperparameters = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-2):\n",
    "    for k in range(10, 40):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        \n",
    "        # Update the best score and hyperparameters if the current score is better\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Ruimtelijk_\"\n",
    "embed_pca_db_ruimtelijk['embed_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organisatorisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pca_db_organisatorisch = Organisatorisch_data.copy()\n",
    "\n",
    "# Generate sentence embeddings for the text variables\n",
    "sentence_embeddings_organisatorisch = model.encode(Organisatorisch_data['text_variables'].tolist())\n",
    "\n",
    "# Combine the sentence embeddings with the other features (assuming they start from column 11)\n",
    "clustering_matrix = np.concatenate((sentence_embeddings_organisatorisch, Organisatorisch_data.iloc[:,11:-1].values), axis=1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "reduced_data = pca.fit_transform(clustering_matrix)\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "best_k = None\n",
    "best_hyperparameters = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-2):\n",
    "    for k in range(10, 40):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        \n",
    "        # Update the best score and hyperparameters if the current score is better\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Organisatorisch_\"\n",
    "embed_pca_db_organisatorisch['embed_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juridisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pca_db_juridisch = Juridisch_data.copy()\n",
    "\n",
    "# Generate sentence embeddings for the text variables\n",
    "sentence_embeddings_juridisch = model.encode(Juridisch_data['text_variables'].tolist())\n",
    "\n",
    "# Combine the sentence embeddings with the other features (assuming they start from column 11)\n",
    "clustering_matrix = np.concatenate((sentence_embeddings_juridisch, Juridisch_data.iloc[:,11:-1].values), axis=1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "reduced_data = pca.fit_transform(clustering_matrix)\n",
    "\n",
    "best_k = None\n",
    "best_hyperparameters = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-2):\n",
    "    for k in range(10, 40):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        \n",
    "        # Update the best score and hyperparameters if the current score is better\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Juridisch_\"\n",
    "embed_pca_db_juridisch['embed_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pca_db_technisch = Technisch_data.copy()\n",
    "\n",
    "# Generate sentence embeddings for the text variables\n",
    "sentence_embeddings_technisch = model.encode(Technisch_data['text_variables'].tolist())\n",
    "\n",
    "# Combine the sentence embeddings with the other features (assuming they start from column 11)\n",
    "clustering_matrix = np.concatenate((sentence_embeddings_technisch, Technisch_data.iloc[:,11:-1].values), axis=1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "reduced_data = pca.fit_transform(clustering_matrix)\n",
    "\n",
    "best_k = None\n",
    "best_hyperparameters = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-2):\n",
    "    for k in range(10, 40):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        \n",
    "        # Update the best score and hyperparameters if the current score is better\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Technisch_\"\n",
    "embed_pca_db_technisch['embed_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financieel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pca_db_financieel = Financieel_data.copy()\n",
    "\n",
    "# Generate sentence embeddings for the text variables\n",
    "sentence_embeddings_financieel = model.encode(Financieel_data['text_variables'].tolist())\n",
    "\n",
    "# Combine the sentence embeddings with the other features (assuming they start from column 11)\n",
    "clustering_matrix = np.concatenate((sentence_embeddings_financieel, Financieel_data.iloc[:,11:-2].values), axis=1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "reduced_data = pca.fit_transform(clustering_matrix)\n",
    "\n",
    "best_k = None\n",
    "best_hyperparameters = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-2):\n",
    "    for k in range(10, 40):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        \n",
    "        # Update the best score and hyperparameters if the current score is better\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"Financieel_\"\n",
    "embed_pca_db_financieel['embed_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pca_db_unknown = unknown_data.copy()\n",
    "\n",
    "# Generate sentence embeddings for the text variables\n",
    "sentence_embeddings_unknown = model.encode(unknown_data['text_variables'].tolist())\n",
    "\n",
    "# Combine the sentence embeddings with the other features (assuming they start from column 11)\n",
    "clustering_matrix = np.concatenate((sentence_embeddings_unknown, unknown_data.iloc[:,11:-2].values), axis=1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "reduced_data = pca.fit_transform(clustering_matrix)\n",
    "\n",
    "best_k = None\n",
    "best_hyperparameters = None\n",
    "best_score = -1\n",
    "best_cluster_labels = None\n",
    "\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-2):\n",
    "    for k in range(10, 40):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=7, n_init=10, max_iter=100)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        \n",
    "        # Update the best score and hyperparameters if the current score is better\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_hyperparameters = {'max_df': max_df, 'min_df': min_df, 'ngram_range': ngram_range, 'norm': norm, 'sublinear_tf': sublinear_tf}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            best_k = k\n",
    "\n",
    "# Print the best hyperparameters and clustering score\n",
    "print(f\"Best hyperparameters: {best_hyperparameters} and best number of clusters: {best_k}\") \n",
    "print(f\"Best silhouette score: {best_score}\") \n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "prefix = \"unknown_\"\n",
    "embed_pca_db_unknown['embed_Cluster_Labels'] = [f\"{prefix}{label}\" for label in best_cluster_labels]\n",
    "\n",
    "# Print the number of clusters and number of elements assigned to each cluster\n",
    "unique_labels, counts = np.unique(best_cluster_labels, return_counts=True)\n",
    "print(f\"Number of clusters: {len(unique_labels)} and number of elements assigned to each cluster: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge them\n",
    "embed_all = pd.concat([embed_pca_db_maatschappelijk, embed_pca_db_politiek, embed_pca_db_ruimtelijk, embed_pca_db_organisatorisch, embed_pca_db_juridisch, embed_pca_db_technisch, embed_pca_db_financieel, embed_pca_db_unknown], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_all.to_excel('Info sheets\\\\embed_all.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# Save the model to a \n",
    "with open('kmeans_model.pkl', 'wb') as file:     \n",
    "    pickle.dump(kmeans, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
